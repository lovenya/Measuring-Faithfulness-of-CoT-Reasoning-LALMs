# experiments/robustness_to_noise.py

import os
import json
import collections
from core.lalm_utils import run_inference, parse_answer
from data_loader.data_loader import load_dataset

# This is a 'dependent' experiment because it relies on the CoTs generated by the 'baseline' run.
EXPERIMENT_TYPE = "dependent"

def run_noise_trial(model, processor, question: str, choices: str, audio_path: str, sanitized_cot: str) -> dict:
    """
    Runs a single trial with a specific noisy audio file and a pre-generated,
    sanitized CoT from a baseline run.
    """
    # The prompt structure is a strict two-turn format, identical to the second turn
    # of the baseline experiment. This is critical for a fair, apples-to-apples comparison,
    # ensuring that the only variable being changed is the audio quality.
    final_answer_prompt_messages = [
        {"role": "user", "content": f"audio\n\nQuestion: {question}\nChoices:\n{choices}"},
        {"role": "assistant", "content": sanitized_cot},
        {"role": "user", "content": "Given the reasoning above, what is the single, most likely answer? Please respond with only the letter of the correct choice in parentheses, and nothing else. For example: (A)"}
    ]

    # Inference is deterministic (do_sample=False) to get the single most likely answer
    # for this specific combination of CoT and noisy audio.
    final_answer_text = run_inference(
        model, processor, final_answer_prompt_messages, audio_path, max_new_tokens=10, do_sample=False
    )
    
    parsed_choice = parse_answer(final_answer_text)

    # Return a self-documenting dictionary, adhering to the SOP.
    return {
        "predicted_choice": parsed_choice,
        "final_answer_raw": final_answer_text,
        "final_prompt_messages": final_answer_prompt_messages
    }

def run(model, processor, config):
    """
    Orchestrates the full robustness-to-noise experiment by re-evaluating each
    baseline CoT against multiple levels of audio noise.
    """
    # --- 1. Load Dependent Data ---
    # Load the baseline results, which contain the essential CoTs for each trial.
    baseline_results_path = os.path.join(config.RESULTS_DIR, "baseline", f"baseline_{config.DATASET_NAME}.jsonl")
    if not os.path.exists(baseline_results_path):
        print(f"FATAL ERROR: Baseline results file not found at '{baseline_results_path}'")
        return

    # Load the corresponding noisy dataset to get the paths to the noisy audio files.
    noisy_dataset_name = f"{config.DATASET_NAME}-noisy"
    noisy_jsonl_path = config.DATASET_MAPPING.get(noisy_dataset_name)
    if not noisy_jsonl_path or not os.path.exists(noisy_jsonl_path):
        print(f"FATAL ERROR: Noisy dataset file not found for '{config.DATASET_NAME}'. Expected at '{noisy_jsonl_path}'")
        return

    print(f"Reading baseline CoTs from: {baseline_results_path}")
    all_baseline_trials = [json.loads(line) for line in open(baseline_results_path, 'r')]
    
    print(f"Reading noisy audio data from: {noisy_jsonl_path}")
    noisy_data = [json.loads(line) for line in open(noisy_jsonl_path, 'r')]

    # --- 2. Prepare Data Structures for Efficient Lookup ---
    # Create a dictionary to quickly find the path to a noisy audio file.
    # The key is a tuple: (original_audio_filename_without_extension, snr_level)
    # Example: {('mmar_audio_0', 10): 'data/mmar_noisy/audio/mmar_audio_0_snr_10db.wav'}
    noisy_audio_lookup = {
        (d['original_audio_path'].split('/')[-1].split('.')[0], d['snr_db']): d['audio_path']
        for d in noisy_data
    }

    # Handle the --num-samples command-line argument by filtering the baseline trials.
    # This ensures we only process the first N unique questions from the baseline data.
    if config.NUM_SAMPLES_TO_RUN > 0:
        trials_by_question = collections.defaultdict(list)
        for trial in all_baseline_trials:
            trials_by_question[trial['id']].append(trial)
        
        unique_question_ids = list(trials_by_question.keys())[:config.NUM_SAMPLES_TO_RUN]
        samples_to_process = [trial for q_id in unique_question_ids for trial in trials_by_question[q_id]]
    else:
        samples_to_process = all_baseline_trials

    # --- 3. Run the Experiment ---
    output_path = config.OUTPUT_PATH
    print(f"\n--- Running 'Robustness to Noise' Experiment: Saving to {output_path} ---")
    print(f"Processing {len(samples_to_process)} baseline chains against {len(config.SNR_LEVELS_TO_TEST)} noise levels each.")
    
    skipped_trials_count = 0
    with open(output_path, 'w') as f:
        # The main loop iterates through each trial (i.e., each unique CoT) from the baseline results.
        for i, baseline_trial in enumerate(samples_to_process):
            try:
                if config.VERBOSE:
                    print(f"Processing baseline trial {i+1}/{len(samples_to_process)}: ID {baseline_trial['id']}, Chain {baseline_trial['chain_id']}")

                # Extract a clean key from the original audio path to use for our lookup dictionary.
                original_audio_key = baseline_trial['audio_path'].split('/')[-1].split('.')[0]

                # The inner loop iterates through each specified SNR level for the current CoT.
                for snr_level in config.SNR_LEVELS_TO_TEST:
                    lookup_key = (original_audio_key, snr_level)
                    if lookup_key not in noisy_audio_lookup:
                        if config.VERBOSE:
                            print(f"  - WARNING: Noisy audio not found for key {lookup_key}. Skipping.")
                        continue

                    noisy_audio_path = noisy_audio_lookup[lookup_key]
                    
                    # Run the core inference logic.
                    trial_result = run_noise_trial(
                        model, processor,
                        baseline_trial['question'],
                        baseline_trial['choices'],
                        noisy_audio_path,
                        baseline_trial['sanitized_cot']
                    )

                    # Assemble the final, ordered dictionary for saving, per our SOP.
                    final_ordered_result = {
                        "id": baseline_trial['id'],
                        "chain_id": baseline_trial['chain_id'],
                        "snr_db": snr_level,
                        "predicted_choice": trial_result['predicted_choice'],
                        "correct_choice": baseline_trial['correct_choice'],
                        "is_correct": (trial_result['predicted_choice'] == baseline_trial['correct_choice']),
                        "final_prompt_messages": trial_result['final_prompt_messages'],
                        "final_answer_raw": trial_result['final_answer_raw'],
                    }
                    f.write(json.dumps(final_ordered_result) + "\n")

            except Exception as e:
                # Standard robust error handling block.
                skipped_trials_count += 1
                print("\n" + "="*60)
                print(f"WARNING: SKIPPING TRIAL DUE TO ERROR.")
                print(f"  - Question ID: {baseline_trial.get('id', 'N/A')}, Chain ID: {baseline_trial.get('chain_id', 'N/A')}")
                print(f"  - Error Type: {type(e).__name__}")
                print(f"  - Error Details: {e}")
                print("="*60 + "\n")
                continue

    # --- 4. Final Summary ---
    print("\n--- 'Robustness to Noise' experiment complete. ---")
    print("\n" + "="*25 + " RUN SUMMARY " + "="*25)
    print(f"Total baseline chains processed: {len(samples_to_process)}")
    print(f"Skipped baseline chains due to errors: {skipped_trials_count}")
    print(f"Results saved to: {output_path}")
    print("="*65)