# Measuring Faithfulness in Chain-of-Thought Reasoning for Large Audio-Language Models

This repository contains the complete framework and results for a series of behavioral experiments designed to measure the reasoning faithfulness of Large Audio-Language Models (LALMs). We investigate the conditions under which a model's generated explanation (its Chain-of-Thought) can be trusted as a true reflection of its decision-making process. Our framework is modular, model-agnostic, and designed for reproducibility in HPC enviroments (online login nodes, offline compute nodes - we're not using any API for inference, downloading all the weights)  

**COMING SOON!! - more models, more datasets**  
**[IN PROGRESS]: "Audio Flamingo 3 - thinking mode" runs are underway, we'll soon add the results**


## Key Findings & Visualizations

A detailed analysis of our findings, including a full gallery of plots and interactive visualizations, is available on our project website.

**[Placeholder for Project Website Link]**

## The Experimental Framework: A Quick Tour

Our methodology is built on a suite of targeted experiments, each designed to probe a specific aspect of reasoning faithfulness. The experiments are divided into two categories:

*   **Foundational Experiments:** These experiments establish the model's performance under normal and zero-reasoning conditions. The `baseline` run is particularly important, as it generates the core Chain-of-Thought (CoT) reasoning that is used in all subsequent tests.
    *   `baseline`
    *   `no_reasoning`

*   **Dependent Experiments:** These experiments are "interventional." They take the reasoning chains generated by the `baseline` run and manipulate them in specific ways to test a hypothesis.
    *   `early_answering` (Probing for Post-Hoc Reasoning)
    *   `paraphrasing` (Meaning vs. Phrasing)
    *   `adding_mistakes` (Sensitivity to Errors)
    *   `filler_text` (Content vs. Compute)
    *   `partial_filler_text` (Corruption from Start)
    *   `flipped_partial_filler_text` (Corruption from End)
    *   `random_partial_filler_text` (Random Corruption)

## Reproducing Our Results

This section provides a complete guide to setting up the environment and replicating our experiments. All experiments were conducted on an HPC cluster using a 40GB slice of an NVIDIA H100 GPU. A GPU with at least 40GB of VRAM is recommended.


### 1. Installation

First, clone this repository to your local machine. Note that this project uses a `git submodule` to include a modified, bug-fixed version of the SALMONN source code. The `--recurse-submodules` flag is the easiest way to ensure you get everything in one command.


```bash
# The --recurse-submodules flag automatically initializes and clones the SALMONN source code submodule.
git clone --recurse-submodules https://github.com/lovenya/Measuring-Faithfulness-of-CoT-Reasoning-LALMs.git
cd Measuring-Faithfulness-of-CoT-Reasoning-LALMs
```

This project supports multiple models, each with its own specific dependencies. You only need to set up the environment for the model you wish to test.

<details>
<summary><b>Setup for Qwen-2 Audio</b></summary>

The environment for Qwen can be set up using the provided requirements file.

```bash
# Create and activate a new virtual environment
python -m venv qwen_env
source qwen_env/bin/activate

# Install dependencies
pip install -r requirements/requirements_qwen.txt
```
</details>

<details>
<summary><b>Setup for SALMONN-13B</b></summary>

SALMONN is a custom research model and requires a specific Python environment.  
Along with Python 3.10, Cuda 12.2 is recommended.  

```bash
# Create and activate a new virtual environment
python3.10 -m venv salmonn_env
source salmonn_env/bin/activate

# Install dependencies
pip install -r requirements/requirements_salmonn.txt
```
</details>


### 2. Model & Data Setup

#### The `config.py` File
**This is the most important file for configuration.** Before running any experiments, you must open `config.py` and ensure that the paths in the `MODEL_PATHS` and `SALMONN_COMPONENT_PATHS` dictionaries point to the correct locations on your filesystem.

#### Model Setup
You must download the pre-trained model weights. The following commands will download the models into the default directories expected by `config.py`. You can also use any location of your choice, as long as you **update the paths in `config.py` accordingly.**

<details>
<summary><b>Setup for Qwen-2 Audio</b></summary>

This command will download the Qwen-2 Audio 7B model weights into a local directory named `qwen2-audio-weights`.

```bash
# Ensure you have Git LFS installed (https://git-lfs.com)
git lfs install

# Clone the model repository
git clone https://huggingface.co/Qwen/Qwen2-Audio-7B-Instruct ./qwen2-audio-weights
```
</details>

<details>
<summary><b>Setup for SALMONN-13B</b></summary>

SALMONN is a multi-component model. You must download each of its four parts and place them in the `./model_components/` directory.

**Step 1: Create the main directory**
```bash
mkdir -p model_components
```

**Step 2: Download Whisper Large v2**
```bash
# Ensure you have Git LFS installed
git lfs install

# Clone the Whisper model
git clone https://huggingface.co/openai/whisper-large-v2 ./model_components/whisper-large-v2
```

**Step 3: Download Vicuna 13B v1.1**
```bash
# Ensure you have Git LFS installed
git lfs install

# Clone the Vicuna model
git clone https://huggingface.co/lmsys/vicuna-13b-v1.1 ./model_components/vicuna-13b-v1.1
```

**Step 4: Download the SALMONN Checkpoint**
```bash
# Ensure you have Git LFS installed
git lfs install

# Clone the SALMONN checkpoint repository
git clone https://huggingface.co/tsinghua-ee/SALMONN ./model_components/salmonn-13b-checkpoint
```

**Step 5: Manually Download the BEATs Checkpoint**
The final component must be downloaded manually from the link provided by the authors, since it is a OneDrive file.

1.  **Download the file:** [Fine-tuned BEATs_iter3+ (AS2M) (cpt2)](https://1drv.ms/u/s!AqeByhGUtINrgcpj8ujXH1YUtxooEg?e=E9Ncea)
2.  **Create the directory:** `mkdir -p ./model_components/beats_iter3_plus_AS2M_finetuned_on_AS2M_cpt2/`
3.  **Move the downloaded file** (`BEATs_iter3_plus_AS2M_finetuned_on_AS2M_cpt2.pt`) into this new directory.

After completing these steps, your `./model_components/` directory will be correctly populated. But it is advised to check and verify once, if the paths are correct.
</details>

#### Data Setup  
The scripts will download the datasets from HuggingFace, download the audios as well, and format the dataset into a standardized '.jsonl' format to ensure the dataset is in ready-to-use format.  
After that please do update the config.py accordingly.  

  
**MMAR**  
```bash
# Ensure that you have enabled an env which has datasets, soundfile, tqdm, requests installed
python -m data_fetch_and_normalisation.download_and_normalize_mmar
```



**SAKURA (All 4 tracks - Animal, Emotion, Gender, Language**  
```bash
# Ensure that you have enabled an env which has datasets, soundfile, tqdm, requests installed
python -m data_fetch_and_normalisation.download_and_normalize_sakura
```

### 3. Flexibility in running the experiments, and HPC friendly settings (Execution details will be included soon)  

1. To save time and compute, and more focused experiment runs, we provide an option to run the experiments in a **`restricted`** setting.
2. To save time, we provide an option to parallelize the runs, where you can split the dataset in as many chunks as you want (say **N**) and later merge the results. Speeds up the process **N**$\times$.
3. We have made the scripts restartable, so it picks right from the last completed result, in order to make it HPC-environment friendly, where one-time time allocation might not be enough, or scripts may get terminated abruptly. Saves hours of re-inference.  

### 4. Running the Experiments

All experiments are launched through our central orchestrator, `main.py`. The script uses a set of flags to control the model, dataset, and experiment to be run.  

Key Command-Line Flags</b>

| Flag | Description |
| :--- | :--- |
| `--model` | **(Required)** The model to use. Your choices are defined in the `MODEL_ALIASES` dictionary in `config.py`.<br>_Choices: `qwen`, `salmonn`_ |
| `--dataset` | **(Required)** The dataset to run on. Your choices are defined in the `DATASET_MAPPING` dictionary in `config.py`.<br>_Choices: `mmar`, `sakura-animal`, `sakura-language`, `sakura-emotion`, `sakura-gender`_ |
| `--experiment`| **(Required)** The experiment to run.<br>_Choices: `baseline`, `no_reasoning`, `filler_text`, `partial_filler_text`, `flipped_partial_filler_text`, `random_partial_filler_text`, `early_answering`, `paraphrasing`, `adding_mistakes`_ |
| `--restricted` | (Optional) Use the pre-filtered dataset (3-6 step CoTs) for a dependent experiment. This significantly reduces runtime. |
| `--num-samples`| (Optional) Run on only the first N samples from the dataset. Ideal for quick debugging runs. |
| `--num-chains` | (Optional) For `baseline` runs, this determines how many reasoning chains to generate per question. For dependent runs, this determines how many of the generated chains to process. |
| `--verbose` | (Optional) Enable detailed, line-by-line progress logging in the console. |
| `--part` | (For parallel runs) The data chunk to process. Used by our Slurm job array scripts. |


#### Example Command:

Here is a complete example command that runs the `adding_mistakes` experiment on the `salmonn` model using the `mmar` dataset. It uses the `restricted` data subset, processes only the first 5 samples, uses the first 3 chains for each, and enables verbose logging.

```bash
python main.py --model salmonn --dataset mmar --experiment adding_mistakes --restricted --num-samples 5 --num-chains 3 --verbose
```

### 5. Generating the Plots

After generating the experimental results, you can use the scripts in the `analysis/` directory to produce the final plots for the paper. These scripts are designed to be flexible and model-agnostic.  
The following are the scripts for the aggregated and grouped (per step) plots. They generate plots for a single dataset, single model at a time.

#### Key Command-Line Flags

| Flag | Description |
| :--- | :--- |
| `--model` | (Required) The model whose results you want to analyze. Choices: `qwen`, `salmonn` |
| `--dataset` | (Required) The dataset to analyze. Can be a single dataset name or `all` to run on all available datasets for the specified model.|
| `--restricted` | (Optional) Analyze the results from a `--restricted` experiment run. The script will look for the `-restricted.jsonl` files.|
| `--grouped` | (Optional) Generate detailed, per-CoT-length plots in addition to the main aggregated plot.|
| `--save-pdf` | (Optional) Save a high-quality PDF copy of each plot in addition to the standard PNG.|
| `--show-...-curve` | (Optional, "Opt-In") Flags to control which metrics are plotted. Example: `--show-consistency-curve` , `--show-accuracy-curve` |
| `--show-...-benchmark` | (Optional, "Opt-In") Flags to control which benchmark lines are shown. Example: `--show-baseline-benchmark`, `--show-nr-benchmark`|

#### Example Command:

Here is a complete example command that generates the **grouped** plots for the **early_answering** experiment on the **salmonn** model for **all available datasets**. It analyzes the **full** (non-restricted) data and plots **all available metrics** (both curves and both benchmarks), saving a PDF copy of each plot.


```bash
python analysis/plot_early_answering.py --model salmonn --dataset all --grouped --show-accuracy-curve --show-consistency-curve --show-baseline-benchmark --show-nr-benchmark --save-pdf
```

===========================================================================  

After generating the experimental results, you can use the scripts in the `analysis/cross_dataset_aggregated_scripts/` directory to produce the final figures presented in our paper.

These scripts are designed to generate a single, high-level plot for each experiment, comparing the model's consistency across all available datasets. They operate exclusively on the `restricted` data subset (1-6 step CoTs) to ensure a focused and efficient analysis.

We currently provide final plotting scripts for our four core interventional experiments:
*   `plot_final_early_answering.py`
*   `plot_final_paraphrasing.py`
*   `plot_final_adding_mistakes.py`
*   `plot_final_random_partial_filler_text.py`

*(Support for the remaining experiments will be added soon.)*

#### **Key Command-Line Flags**

These scripts share a common, streamlined set of flags for controlling the output.

| Flag | Description |
| :--- | :--- |
| `--model` | **(Required)** The model whose results you want to analyze.<br>_Choices: `qwen`, `salmonn`_ |
| `--y-zoom <MIN> <MAX>` | (Optional) Set a custom Y-axis range for the plot.<br>_Example: `--y-zoom 45 100.5`_ |
| `--show-ci` | (Optional) Render the 95% confidence interval as a shaded region around the mean. |
| `--save-pdf` | (Optional) Save a high-quality PDF copy of the plot in addition to the standard PNG. |
| `--print-line-data` | (Optional) Print the final aggregated X and Y coordinates for each dataset's line to the console. |
| `--save-stats` | (Optional) Save a detailed statistical summary (including per-bin distributional stats) to a `.txt` file. |

#### **Example Command:**

Here is a complete example command that generates the final cross-dataset plot for the **paraphrasing** experiment on the **qwen** model. It enables the confidence interval shading, saves a PDF copy, and zooms the y-axis to the 45-100% range.

```bash
python analysis/cross_dataset_aggregated_scripts/plot_final_paraphrasing.py --model qwen --show-ci --save-pdf --y-zoom 45 100.5
```


