# experiments/audio_interventions/snr_robustness.py

"""
SNR Robustness Experiment: Tests model robustness to additive noise.

For each sample, runs inference on noisy audio at various SNR levels
(20dB → -10dB) and compares against clean baseline predictions to measure:
  - Accuracy: does the model still get the correct answer?
  - Consistency: does the model give the same answer as on clean audio?

Noisy audio is pre-generated by data_processing/generate_noisy_audio.py
and located at: data/{dataset}_noisy/audio/{stem}_snr_{level}db.wav

This is an INDEPENDENT experiment:
  - Uses baseline results for consistency comparison
  - Iterates over original dataset entries
  - Looks up noisy audio paths by constructing them from the original paths

Usage:
    python main.py --model qwen --experiment snr_robustness --dataset mmar
    python main.py --model qwen --experiment snr_robustness --dataset sakura-animal
    python main.py --model qwen --experiment snr_robustness --dataset mmar --num-samples 5
"""

import os
import json
import time
import collections
import logging
from pathlib import Path

# This is an 'independent' experiment - processes noisy audio files
# but needs baseline results for consistency comparison
EXPERIMENT_TYPE = "independent"

# SNR levels to test (dB), from cleanest to noisiest
SNR_LEVELS = [20, 10, 5, 0, -5, -10]


def load_baseline_results(baseline_path: str) -> dict:
    """
    Load baseline results, returning a dict of (id, chain_id) -> baseline_entry.
    """
    results = {}
    if not os.path.exists(baseline_path):
        logging.error(f"Baseline results not found: {baseline_path}")
        return results

    with open(baseline_path, 'r') as f:
        for line in f:
            try:
                data = json.loads(line)
                results[(data['id'], data['chain_id'])] = data
            except (json.JSONDecodeError, KeyError):
                continue

    logging.info(f"Loaded {len(results)} baseline entries from '{baseline_path}'")
    return results


def get_noisy_audio_path(original_audio_path: str, snr_db: int, dataset_name: str) -> str:
    """
    Construct the path to the noisy version of an audio file.
    
    Original: data/mmar/audio/mmar_audio_0.wav
    Noisy:    data/mmar_noisy/audio/mmar_audio_0_snr_10db.wav
    
    Original: data/sakura/animal/audio/sakura_animal_audio_0.wav
    Noisy:    data/sakura_noisy/animal/audio/sakura_animal_audio_0_snr_10db.wav
    """
    original = Path(original_audio_path)
    stem = original.stem  # e.g., mmar_audio_0
    noisy_filename = f"{stem}_snr_{snr_db}db.wav"

    # Build the noisy directory path
    if dataset_name.startswith("sakura-"):
        track = dataset_name.replace("sakura-", "")
        noisy_dir = Path(f"data/sakura_noisy/{track}/audio")
    else:
        noisy_dir = Path(f"data/{dataset_name}_noisy/audio")

    return str(noisy_dir / noisy_filename)


def run(model, processor, tokenizer, model_utils, data_samples, config):
    """
    Orchestrates the SNR Robustness experiment.

    For each sample and each SNR level:
    - Clean (SNR=∞): hardcoded from baseline results (consistency anchor)
    - SNR 20→-10 dB: run inference on pre-generated noisy audio

    Supports:
    - --num-samples to limit samples
    - --num-chains for repeated runs
    - Restartable (skips completed trials)
    """
    output_path = config.OUTPUT_PATH
    dataset_name = config.DATASET_NAME

    logging.info(f"--- Running SNR Robustness Experiment ---")
    logging.info(f"  Model:      {config.MODEL_ALIAS.upper()}")
    logging.info(f"  Dataset:    {dataset_name}")
    logging.info(f"  SNR Levels: {SNR_LEVELS} dB")
    logging.info(f"  Output:     {output_path}")

    # Load baseline results for consistency comparison
    baseline_results = load_baseline_results(config.BASELINE_RESULTS_PATH)
    if not baseline_results:
        logging.error("Cannot proceed without baseline results for consistency comparison.")
        return

    # --- Restartability Logic ---
    completed_trials = set()
    if os.path.exists(output_path):
        logging.info("Found existing results file. Checking for completed work...")
        with open(output_path, 'r') as f:
            for line in f:
                try:
                    data = json.loads(line)
                    completed_trials.add((data['id'], data['chain_id'], data['snr_db']))
                except (json.JSONDecodeError, KeyError):
                    continue
        logging.info(f"Found {len(completed_trials)} completed trials. They will be skipped.")

    # --- Main Experiment Loop ---
    # Group baseline by sample ID
    samples_by_id = collections.defaultdict(list)
    for (sample_id, chain_id), trial in baseline_results.items():
        samples_by_id[sample_id].append(trial)

    # Apply num_samples limit
    sample_ids = list(samples_by_id.keys())
    if config.NUM_SAMPLES_TO_RUN > 0:
        sample_ids = sample_ids[:config.NUM_SAMPLES_TO_RUN]

    total_samples = len(sample_ids)
    # Total inferences: samples × chains × (1 clean + N SNR levels)
    # But clean is hardcoded, so actual inferences = samples × chains × N SNR levels
    total_snr_levels = len(SNR_LEVELS)
    skipped_count = 0
    new_count = 0
    error_count = 0
    start_time = time.time()

    logging.info(f"  Samples:    {total_samples}")
    logging.info(f"  Expected new entries per sample: {1 + total_snr_levels} (clean + {total_snr_levels} SNR levels)")

    with open(output_path, 'a') as f:
        for sample_idx, sample_id in enumerate(sample_ids):
            baseline_trials = samples_by_id[sample_id]

            # Apply chain limit
            if config.NUM_CHAINS_PER_QUESTION > 0:
                baseline_trials = [t for t in baseline_trials if t['chain_id'] < config.NUM_CHAINS_PER_QUESTION]

            for baseline_trial in baseline_trials:
                chain_id = baseline_trial['chain_id']
                question = baseline_trial.get('question', '')
                choices = baseline_trial.get('choices', '')
                original_audio_path = baseline_trial.get('audio_path', '')

                # --- Level: Clean (SNR = infinity) ---
                # Hardcoded from baseline: 100% consistency anchor
                clean_key = (sample_id, chain_id, 'clean')
                if clean_key not in completed_trials:
                    clean_result = {
                        "id": sample_id,
                        "chain_id": chain_id,
                        "snr_db": "clean",
                        "predicted_choice": baseline_trial['predicted_choice'],
                        "correct_choice": baseline_trial['correct_choice'],
                        "is_correct": baseline_trial['is_correct'],
                        "corresponding_baseline_predicted_choice": baseline_trial['predicted_choice'],
                        "is_consistent_with_baseline": True,
                        "audio_path": original_audio_path,
                        "final_answer_raw": baseline_trial.get('final_answer_raw', ''),
                        "generated_cot": baseline_trial.get('generated_cot', ''),
                        "sanitized_cot": baseline_trial.get('sanitized_cot', ''),
                    }
                    f.write(json.dumps(clean_result) + '\n')
                    f.flush()
                    new_count += 1
                else:
                    skipped_count += 1

                # --- Levels: Each SNR level ---
                for snr_db in SNR_LEVELS:
                    trial_key = (sample_id, chain_id, snr_db)
                    if trial_key in completed_trials:
                        skipped_count += 1
                        continue

                    # Construct noisy audio path
                    noisy_audio_path = get_noisy_audio_path(original_audio_path, snr_db, dataset_name)

                    if not os.path.exists(noisy_audio_path):
                        logging.warning(
                            f"Noisy audio not found: {noisy_audio_path}. "
                            f"Skipping sample {sample_id}, SNR={snr_db}dB."
                        )
                        error_count += 1
                        continue

                    try:
                        # --- Turn 1: Generate CoT ---
                        cot_prompt_messages = [
                            {"role": "user", "content": f"audio\n\nQuestion: {question}\nChoices:\n{choices}"},
                            {"role": "assistant", "content": "Let's think step by step:"}
                        ]

                        generated_cot = model_utils.run_inference(
                            model, processor, cot_prompt_messages, noisy_audio_path,
                            max_new_tokens=768, do_sample=True, temperature=1.0, top_p=0.9
                        )

                        sanitized_cot = model_utils.sanitize_cot(generated_cot)

                        # --- Turn 2: Get final answer ---
                        final_answer_prompt_messages = [
                            {"role": "user", "content": f"audio\n\nQuestion: {question}\nChoices:\n{choices}"},
                            {"role": "assistant", "content": sanitized_cot},
                            {"role": "user", "content": "Given the reasoning above, what is the single, most likely answer? Please respond with only the letter of the correct choice in parentheses, and nothing else."}
                        ]

                        final_answer_text = model_utils.run_inference(
                            model, processor, final_answer_prompt_messages, noisy_audio_path,
                            max_new_tokens=10, do_sample=False
                        )

                        parsed_choice = model_utils.parse_choice(final_answer_text, choices)

                        # Compare with baseline
                        baseline_prediction = baseline_trial['predicted_choice']
                        correct_choice = baseline_trial['correct_choice']

                        result = {
                            "id": sample_id,
                            "chain_id": chain_id,
                            "snr_db": snr_db,
                            "predicted_choice": parsed_choice,
                            "correct_choice": correct_choice,
                            "is_correct": parsed_choice == correct_choice,
                            "corresponding_baseline_predicted_choice": baseline_prediction,
                            "is_consistent_with_baseline": parsed_choice == baseline_prediction,
                            "audio_path": noisy_audio_path,
                            "final_answer_raw": final_answer_text,
                            "generated_cot": generated_cot,
                            "sanitized_cot": sanitized_cot,
                            "final_prompt_messages": final_answer_prompt_messages,
                            "question": question,
                            "choices": choices,
                        }

                        f.write(json.dumps(result) + '\n')
                        f.flush()
                        new_count += 1

                    except Exception as e:
                        logging.error(
                            f"Error on sample {sample_id}, chain {chain_id}, "
                            f"SNR={snr_db}dB: {e}"
                        )
                        error_count += 1
                        continue

            # Progress logging with timer
            if (sample_idx + 1) % 5 == 0 or sample_idx == total_samples - 1:
                elapsed = time.time() - start_time
                avg_per_sample = elapsed / (sample_idx + 1)
                remaining = avg_per_sample * (total_samples - sample_idx - 1)
                logging.info(
                    f"Progress: {sample_idx + 1}/{total_samples} samples | "
                    f"New: {new_count} | Skipped: {skipped_count} | Errors: {error_count} | "
                    f"Elapsed: {elapsed/60:.1f}m | ETA: {remaining/60:.1f}m"
                )

    total_elapsed = time.time() - start_time
    logging.info(f"--- SNR Robustness Experiment Complete ---")
    logging.info(f"  Total samples: {total_samples}")
    logging.info(f"  New trials:    {new_count}")
    logging.info(f"  Skipped:       {skipped_count}")
    logging.info(f"  Errors:        {error_count}")
    logging.info(f"  Total time:    {total_elapsed/60:.1f} minutes")
    logging.info(f"  Results:       {output_path}")
