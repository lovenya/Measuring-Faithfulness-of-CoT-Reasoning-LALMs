# Measuring Faithfulness in Chain-of-Thought Reasoning for Large Audio-Language Models

This repository contains the complete framework and results for a series of behavioral experiments designed to measure the reasoning faithfulness of Large Audio-Language Models (LALMs). We investigate the conditions under which a model's generated explanation (its Chain-of-Thought) can be trusted as a true reflection of its decision-making process. Our framework is modular, model-agnostic, and designed for reproducibility in HPC enviroments (online login nodes, offline compute nodes - we're not using any API for inference, downloading all the weights)

## Key Findings & Visualizations

A detailed analysis of our findings, including a full gallery of plots and interactive visualizations, is available on our project website.

**[Placeholder for Project Website Link]**

## The Experimental Framework: A Quick Tour

Our methodology is built on a suite of targeted experiments, each designed to probe a specific aspect of reasoning faithfulness. The experiments are divided into two categories:

*   **Foundational Experiments:** These experiments establish the model's performance under normal and zero-reasoning conditions. The `baseline` run is particularly important, as it generates the core Chain-of-Thought (CoT) reasoning that is used in all subsequent tests.
    *   `baseline`
    *   `no_reasoning`

*   **Dependent Experiments:** These experiments are "interventional." They take the reasoning chains generated by the `baseline` run and manipulate them in specific ways to test a hypothesis.
    *   `early_answering` (Probing for Post-Hoc Reasoning)
    *   `paraphrasing` (Meaning vs. Phrasing)
    *   `adding_mistakes` (Sensitivity to Errors)
    *   `filler_text` (Content vs. Compute)
    *   `partial_filler_text` (Corruption from Start)
    *   `flipped_partial_filler_text` (Corruption from End)
    *   `random_partial_filler_text` (Random Corruption)

## Reproducing Our Results

This section provides a complete guide to setting up the environment and replicating our experiments. All experiments were conducted on an HPC cluster using a 40GB slice of an NVIDIA H100 GPU. A GPU with at least 40GB of VRAM is recommended.


### 1. Installation

First, clone this repository to your local machine:

```bash
git clone https://github.com/lovenya/Measuring-Faithfulness-of-CoT-Reasoning-LALMs.git
cd Measuring-Faithfulness-of-CoT-Reasoning-LALMs
```

This project supports multiple models, each with its own specific dependencies. You only need to set up the environment for the model you wish to test.

<details>
<summary><b>Environment Setup for Qwen-2-Audio-7B</b></summary>

The environment for Qwen can be set up using the provided requirements file.

```bash
# Create and activate a new virtual environment
python -m venv qwen_env
source qwen_env/bin/activate

# Install dependencies
pip install -r requirements/requirements_qwen.txt
```
</details>

<details>
<summary><b>Environment Setup for SALMONN-13B</b></summary>

SALMONN is a custom research model and requires a more detailed setup.

**Step 1: Clone the SALMONN Source Code**
The model's code is a required dependency. We add it to our project as a `git submodule` to lock it to the specific version we used.

```bash
# From the root of this project, run:
git submodule add --branch salmonn --depth 1 https://github.com/bytedance/SALMONN.git ./salmonn-source-code
```

**Step 2: Set up the Python Environment (Python 3.9/3.10 is recommended)**
```bash
# Create and activate a new virtual environment
python3.10 -m venv salmonn_env
source salmonn_env/bin/activate

# Install dependencies
pip install -r requirements/requirements_salmonn.txt
```
</details>


### 2. Model & Data Setup

#### The `config.py` File
**This is the most important file for configuration.** Before running any experiments, you must open `config.py` and ensure that the paths in the `MODEL_PATHS` and `SALMONN_COMPONENT_PATHS` dictionaries point to the correct locations on your filesystem.

#### Model Setup
You must download the pre-trained model weights. The following commands will download the models into the default directories expected by `config.py`. You can also use any location of your choice, as long as you **update the paths in `config.py` accordingly.**

<details>
<summary><b>Setup for Qwen-2 Audio</b></summary>

This command will download the Qwen-2 Audio 7B model weights into a local directory named `qwen2-audio-weights`.

```bash
# Ensure you have Git LFS installed (https://git-lfs.com)
git lfs install

# Clone the model repository
git clone https://huggingface.co/Qwen/Qwen2-Audio-7B-Instruct ./qwen2-audio-weights
```
</details>

<details>
<summary><b>Setup for SALMONN-13B</b></summary>

SALMONN is a multi-component model. You must download each of its four parts and place them in the `./model_components/` directory.

**Step 1: Create the main directory**
```bash
mkdir -p model_components
```

**Step 2: Download Whisper Large v2**
```bash
# Ensure you have Git LFS installed
git lfs install

# Clone the Whisper model
git clone https://huggingface.co/openai/whisper-large-v2 ./model_components/whisper-large-v2
```

**Step 3: Download Vicuna 13B v1.1**
```bash
# Ensure you have Git LFS installed
git lfs install

# Clone the Vicuna model
git clone https://huggingface.co/lmsys/vicuna-13b-v1.1 ./model_components/vicuna-13b-v1.1
```

**Step 4: Download the SALMONN Checkpoint**
```bash
# Create the specific subdirectory
mkdir -p ./model_components/salmonn-13b-checkpoint

# Download the checkpoint file into it
wget -P ./model_components/salmonn-13b-checkpoint/ https://huggingface.co/tsinghua-ee/SALMONN/resolve/main/salmonn_v1.pth
```

**Step 5: Manually Download the BEATs Checkpoint**
The final component must be downloaded manually from the link provided by the authors, since it is a OneDrive file.

1.  **Download the file:** [Fine-tuned BEATs_iter3+ (AS2M) (cpt2)](https://1drv.ms/u/s!AqeByhGUtINrgcpj8ujXH1YUtxooEg?e=E9Ncea)
2.  **Create the directory:** `mkdir -p ./model_components/beats_iter3_plus_AS2M_finetuned_on_AS2M_cpt2/`
3.  **Move the downloaded file** (`BEATs_iter3_plus_AS2M_finetuned_on_AS2M_cpt2.pt`) into this new directory.

After completing these steps, your `./model_components/` directory will be correctly populated. But it is advised to check and verify once, if the paths are correct.
</details>

#### Data Setup
**(Coming Soon)**

We will be providing the pre-processing scripts used to convert the raw MMAR and Sakura datasets into our standardized `.jsonl` format. For now, please ensure your data is structured to match the paths defined in the `DATASET_MAPPING` dictionary in `config.py`.

### 3. Running the Experiments

All experiments are launched through our central orchestrator, `main.py`. The script uses a set of flags to control the model, dataset, and experiment to be run.  

Key Command-Line Flags</b>

| Flag | Description |
| :--- | :--- |
| `--model` | **(Required)** The model to use. Your choices are defined in the `MODEL_ALIASES` dictionary in `config.py`.<br>_Choices: `qwen`, `salmonn`_ |
| `--dataset` | **(Required)** The dataset to run on. Your choices are defined in the `DATASET_MAPPING` dictionary in `config.py`.<br>_Choices: `mmar`, `sakura-animal`, `sakura-language`, `sakura-emotion`, `sakura-gender`_ |
| `--experiment`| **(Required)** The experiment to run.<br>_Choices: `baseline`, `no_reasoning`, `no_cot`, `filler_text`, `partial_filler_text`, `flipped_partial_filler_text`, `random_partial_filler_text`, `early_answering`, `paraphrasing`, `adding_mistakes`, `robustness_to_noise`_ |
| `--restricted` | (Optional) Use the pre-filtered dataset (3-6 step CoTs) for a dependent experiment. This significantly reduces runtime. |
| `--num-samples`| (Optional) Run on only the first N samples from the dataset. Ideal for quick debugging runs. |
| `--num-chains` | (Optional) For `baseline` runs, this determines how many reasoning chains to generate per question. For dependent runs, this determines how many of the generated chains to process. |
| `--verbose` | (Optional) Enable detailed, line-by-line progress logging in the console. |
| `--part` | (For parallel runs) The data chunk to process. Used by our Slurm job array scripts. |


#### Example Command:

Here is a complete example command that runs the `adding_mistakes` experiment on the `salmonn` model using the `mmar` dataset. It uses the `restricted` data subset, processes only the first 5 samples, uses the first 3 chains for each, and enables verbose logging.

<details>
<summary>Click to expand code</summary>

```bash
python main.py \
    --model salmonn \
    --dataset mmar \
    --experiment adding_mistakes \
    --restricted \
    --num-samples 5 \
    --num-chains 3 \
    --verbose
```
</details>

### 5. Generating the Plots

**(Coming Soon)**

Instructions on how to use the scripts in the `analysis/` directory to generate the final plots from the results files will be added shortly.
