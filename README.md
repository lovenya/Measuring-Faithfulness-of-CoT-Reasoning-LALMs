# Measuring Faithfulness in Chain-of-Thought Reasoning for Large Audio-Language Models

This repository contains the complete framework and results for a series of behavioral experiments designed to measure the reasoning faithfulness of Large Audio-Language Models (LALMs). We investigate the conditions under which a model's generated explanation (its Chain-of-Thought) can be trusted as a true reflection of its decision-making process. Our framework is modular, model-agnostic, and designed for reproducibility in HPC enviroments (online login nodes, offline compute nodes - we're not using any API for inference, downloading all the weights)  

**COMING SOON!! - more models, more datasets**
**[IN PROGRESS]: Working on adding Audio Flamingo 3**
**[IN PROGRESS]: Worknig on batch infrence**  


## Key Findings & Visualizations

A detailed analysis of our findings, including a full gallery of plots and interactive visualizations, is available on our project website.

**[Placeholder for Project Website Link]**

## The Experimental Framework: A Quick Tour

Our methodology is built on a suite of targeted experiments, each designed to probe a specific aspect of reasoning faithfulness. The experiments are divided into two categories:

*   **Foundational Experiments:** These experiments establish the model's performance under normal and zero-reasoning conditions. The `baseline` run is particularly important, as it generates the core Chain-of-Thought (CoT) reasoning that is used in all subsequent tests.
    *   `baseline`
    *   `no_reasoning`

*   **Dependent Experiments:** These experiments are "interventional." They take the reasoning chains generated by the `baseline` run and manipulate them in specific ways to test a hypothesis.
    *   `early_answering` (Probing for Post-Hoc Reasoning)
    *   `paraphrasing` (Meaning vs. Phrasing)
    *   `adding_mistakes` (Sensitivity to Errors)
    *   `filler_text` (Content vs. Compute)
    *   `partial_filler_text` (Corruption from Start)
    *   `flipped_partial_filler_text` (Corruption from End)
    *   `random_partial_filler_text` (Random Corruption)

## Reproducing Our Results

This section provides a complete guide to setting up the environment and replicating our experiments. All experiments were conducted on an HPC cluster using a 40GB slice of an NVIDIA H100 GPU. A GPU with at least 40GB of VRAM is recommended.


### 1. Installation

First, clone this repository to your local machine. Note that this project uses a `git submodule` to include a modified, bug-fixed version of the SALMONN source code. The `--recurse-submodules` flag is the easiest way to ensure you get everything in one command.


```bash
# The --recurse-submodules flag automatically initializes and clones the SALMONN source code submodule.
git clone --recurse-submodules https://github.com/lovenya/Measuring-Faithfulness-of-CoT-Reasoning-LALMs.git
cd Measuring-Faithfulness-of-CoT-Reasoning-LALMs
```

This project supports multiple models, each with its own specific dependencies. You only need to set up the environment for the model you wish to test.

<details>
<summary><b>Setup for Qwen-2 Audio</b></summary>

The environment for Qwen can be set up using the provided requirements file.

```bash
# Create and activate a new virtual environment
python -m venv qwen_env
source qwen_env/bin/activate

# Install dependencies
pip install -r requirements/requirements_qwen.txt
```
</details>

<details>
<summary><b>Setup for SALMONN-13B</b></summary>

SALMONN is a custom research model and requires a specific Python environment.  
Along with Python 3.10, Cuda 12.2 is recommended.  

```bash
# Create and activate a new virtual environment
python3.10 -m venv salmonn_env
source salmonn_env/bin/activate

# Install dependencies
pip install -r requirements/requirements_salmonn.txt
```
</details>


### 2. Model & Data Setup

#### The `config.py` File
**This is the most important file for configuration.** Before running any experiments, you must open `config.py` and ensure that the paths in the `MODEL_PATHS` and `SALMONN_COMPONENT_PATHS` dictionaries point to the correct locations on your filesystem.

#### Model Setup
You must download the pre-trained model weights. The following commands will download the models into the default directories expected by `config.py`. You can also use any location of your choice, as long as you **update the paths in `config.py` accordingly.**

<details>
<summary><b>Setup for Qwen-2 Audio</b></summary>

This command will download the Qwen-2 Audio 7B model weights into a local directory named `qwen2-audio-weights`.

```bash
# Ensure you have Git LFS installed (https://git-lfs.com)
git lfs install

# Clone the model repository
git clone https://huggingface.co/Qwen/Qwen2-Audio-7B-Instruct ./qwen2-audio-weights
```
</details>

<details>
<summary><b>Setup for SALMONN-13B</b></summary>

SALMONN is a multi-component model. You must download each of its four parts and place them in the `./model_components/` directory.

**Step 1: Create the main directory**
```bash
mkdir -p model_components
```

**Step 2: Download Whisper Large v2**
```bash
# Ensure you have Git LFS installed
git lfs install

# Clone the Whisper model
git clone https://huggingface.co/openai/whisper-large-v2 ./model_components/whisper-large-v2
```

**Step 3: Download Vicuna 13B v1.1**
```bash
# Ensure you have Git LFS installed
git lfs install

# Clone the Vicuna model
git clone https://huggingface.co/lmsys/vicuna-13b-v1.1 ./model_components/vicuna-13b-v1.1
```

**Step 4: Download the SALMONN Checkpoint**
```bash
# Ensure you have Git LFS installed
git lfs install

# Clone the SALMONN checkpoint repository
git clone https://huggingface.co/tsinghua-ee/SALMONN ./model_components/salmonn-13b-checkpoint
```

**Step 5: Manually Download the BEATs Checkpoint**
The final component must be downloaded manually from the link provided by the authors, since it is a OneDrive file.

1.  **Download the file:** [Fine-tuned BEATs_iter3+ (AS2M) (cpt2)](https://1drv.ms/u/s!AqeByhGUtINrgcpj8ujXH1YUtxooEg?e=E9Ncea)
2.  **Create the directory:** `mkdir -p ./model_components/beats_iter3_plus_AS2M_finetuned_on_AS2M_cpt2/`
3.  **Move the downloaded file** (`BEATs_iter3_plus_AS2M_finetuned_on_AS2M_cpt2.pt`) into this new directory.

After completing these steps, your `./model_components/` directory will be correctly populated. But it is advised to check and verify once, if the paths are correct.
</details>

#### Data Setup
**(Coming Soon)**

We will be providing the pre-processing scripts used to convert the raw MMAR and Sakura datasets into our standardized `.jsonl` format. For now, please ensure your data is structured to match the paths defined in the `DATASET_MAPPING` dictionary in `config.py`.

### 3. Running the Experiments

All experiments are launched through our central orchestrator, `main.py`. The script uses a set of flags to control the model, dataset, and experiment to be run.  

Key Command-Line Flags</b>

| Flag | Description |
| :--- | :--- |
| `--model` | **(Required)** The model to use. Your choices are defined in the `MODEL_ALIASES` dictionary in `config.py`.<br>_Choices: `qwen`, `salmonn`_ |
| `--dataset` | **(Required)** The dataset to run on. Your choices are defined in the `DATASET_MAPPING` dictionary in `config.py`.<br>_Choices: `mmar`, `sakura-animal`, `sakura-language`, `sakura-emotion`, `sakura-gender`_ |
| `--experiment`| **(Required)** The experiment to run.<br>_Choices: `baseline`, `no_reasoning`, `filler_text`, `partial_filler_text`, `flipped_partial_filler_text`, `random_partial_filler_text`, `early_answering`, `paraphrasing`, `adding_mistakes`_ |
| `--restricted` | (Optional) Use the pre-filtered dataset (3-6 step CoTs) for a dependent experiment. This significantly reduces runtime. |
| `--num-samples`| (Optional) Run on only the first N samples from the dataset. Ideal for quick debugging runs. |
| `--num-chains` | (Optional) For `baseline` runs, this determines how many reasoning chains to generate per question. For dependent runs, this determines how many of the generated chains to process. |
| `--verbose` | (Optional) Enable detailed, line-by-line progress logging in the console. |
| `--part` | (For parallel runs) The data chunk to process. Used by our Slurm job array scripts. |


#### Example Command:

Here is a complete example command that runs the `adding_mistakes` experiment on the `salmonn` model using the `mmar` dataset. It uses the `restricted` data subset, processes only the first 5 samples, uses the first 3 chains for each, and enables verbose logging.

```bash
python main.py --model salmonn --dataset mmar --experiment adding_mistakes --restricted --num-samples 5 --num-chains 3 --verbose
```

### 4. Generating the Plots

After generating the experimental results, you can use the scripts in the `analysis/` directory to produce the final plots for the paper. These scripts are designed to be flexible and model-agnostic.  
The following are the scripts for the aggregated and grouped (per step) plots. They generate plots for a single dataset, single model at a time.

#### Key Command-Line Flags

| Flag | Description |
| :--- | :--- |
| `--model` | (Required) The model whose results you want to analyze. Choices: `qwen`, `salmonn` |
| `--dataset` | (Required) The dataset to analyze. Can be a single dataset name or `all` to run on all available datasets for the specified model.|
| `--restricted` | (Optional) Analyze the results from a `--restricted` experiment run. The script will look for the `-restricted.jsonl` files.|
| `--grouped` | (Optional) Generate detailed, per-CoT-length plots in addition to the main aggregated plot.|
| `--save-pdf` | (Optional) Save a high-quality PDF copy of each plot in addition to the standard PNG.|
| `--show-...-curve` | (Optional, "Opt-In") Flags to control which metrics are plotted. Example: `--show-consistency-curve` , `--show-accuracy-curve` |
| `--show-...-benchmark` | (Optional, "Opt-In") Flags to control which benchmark lines are shown. Example: `--show-baseline-benchmark`, `--show-nr-benchmark`|

#### Example Command:

Here is a complete example command that generates the **grouped** plots for the **early_answering** experiment on the **salmonn** model for **all available datasets**. It analyzes the **full** (non-restricted) data and plots **all available metrics** (both curves and both benchmarks), saving a PDF copy of each plot.


```bash
python analysis/plot_early_answering.py --model salmonn --dataset all --grouped --show-accuracy-curve --show-consistency-curve --show-baseline-benchmark --show-nr-benchmark --save-pdf
```

===========================================================================  

After generating the experimental results, you can use the scripts in the `analysis/cross_dataset_aggregated_scripts/` directory to produce the final figures presented in our paper.

These scripts are designed to generate a single, high-level plot for each experiment, comparing the model's consistency across all available datasets. They operate exclusively on the `restricted` data subset (1-6 step CoTs) to ensure a focused and efficient analysis.

We currently provide final plotting scripts for our four core interventional experiments:
*   `plot_final_early_answering.py`
*   `plot_final_paraphrasing.py`
*   `plot_final_adding_mistakes.py`
*   `plot_final_random_partial_filler_text.py`

*(Support for the remaining experiments will be added soon.)*

#### **Key Command-Line Flags**

These scripts share a common, streamlined set of flags for controlling the output.

| Flag | Description |
| :--- | :--- |
| `--model` | **(Required)** The model whose results you want to analyze.<br>_Choices: `qwen`, `salmonn`_ |
| `--y-zoom <MIN> <MAX>` | (Optional) Set a custom Y-axis range for the plot.<br>_Example: `--y-zoom 45 100.5`_ |
| `--show-ci` | (Optional) Render the 95% confidence interval as a shaded region around the mean. |
| `--save-pdf` | (Optional) Save a high-quality PDF copy of the plot in addition to the standard PNG. |
| `--print-line-data` | (Optional) Print the final aggregated X and Y coordinates for each dataset's line to the console. |
| `--save-stats` | (Optional) Save a detailed statistical summary (including per-bin distributional stats) to a `.txt` file. |

#### **Example Command:**

Here is a complete example command that generates the final cross-dataset plot for the **paraphrasing** experiment on the **qwen** model. It enables the confidence interval shading, saves a PDF copy, and zooms the y-axis to the 45-100% range.

```bash
python analysis/cross_dataset_aggregated_scripts/plot_final_paraphrasing.py --model qwen --show-ci --save-pdf --y-zoom 45 100.5
```


