# core/filler_text_utils.py

"""
This is a shared utility module for all 'filler text' type experiments.
It provides centralized, model-agnostic functions for creating and running
trials with modified reasoning chains.
"""

import random
import nltk
import json
import os

def load_lorem_token_pool() -> list:
    """
    Load pre-computed Lorem Ipsum word pool.
    
    The pool is pre-generated by scripts/generate_lorem_tokens.py
    to avoid any runtime overhead.
    
    Returns:
        list[str]: A list of Lorem Ipsum words.
    
    Raises:
        FileNotFoundError: If the token pool file doesn't exist.
    """
    pool_path = os.path.join(
        os.path.dirname(os.path.dirname(__file__)),
        "assets", "lorem_tokens", "lorem_tokens.json"
    )
    
    if not os.path.exists(pool_path):
        raise FileNotFoundError(
            f"Lorem token pool not found. "
            f"Please run: python scripts/generate_lorem_tokens.py"
        )
    
    with open(pool_path, 'r') as f:
        data = json.load(f)
    
    return data['tokens']

def create_word_level_masked_cot(cot_text: str, percentile: int, mode: str, filler_type: str = 'dots', lorem_pool: list = None) -> str:
    """
    Creates a modified CoT by replacing a percentage of WORDS with filler text.

    This function uses nltk.word_tokenize to ensure that we are operating on
    the word level, which is the established methodology for this experiment
    and is crucial for maintaining consistency with previously generated results.

    Args:
        cot_text (str): The sanitized Chain-of-Thought.
        percentile (int): The percentage of words to replace (0-100).
        mode (str): The method of replacement ('start', 'end', or 'random').
        filler_type (str): Type of filler - 'dots' for '...' or 'lorem' for Lorem Ipsum tokens.
        lorem_pool (list): Pre-computed list of Lorem Ipsum tokens (required if filler_type='lorem').

    Returns:
        str: The CoT with words replaced by filler text.
    """
    
    # We use nltk.word_tokenize for a robust, word-level split.
    words = nltk.word_tokenize(cot_text)
    
    total_words = len(words)
    if total_words == 0:
        return ""

    num_to_replace = int((percentile / 100) * total_words)
    
    def get_filler():
        """Helper to get a filler token based on filler_type."""
        if filler_type == 'lorem' and lorem_pool:
            return random.choice(lorem_pool)
        return "..."

    if mode == 'start':
        if num_to_replace > 0:
            if filler_type == 'lorem' and lorem_pool:
                filler_tokens = [random.choice(lorem_pool) for _ in range(num_to_replace)]
                new_words = filler_tokens + words[num_to_replace:]
            else:
                new_words = ["..."] + words[num_to_replace:]
        else:
            new_words = words
    elif mode == 'end':
        if num_to_replace > 0:
            if filler_type == 'lorem' and lorem_pool:
                filler_tokens = [random.choice(lorem_pool) for _ in range(num_to_replace)]
                new_words = words[:-num_to_replace] + filler_tokens
            else:
                new_words = words[:-num_to_replace] + ["..."]
        else:
            new_words = words
    elif mode == 'random':
        indices_to_replace = set(random.sample(range(total_words), num_to_replace))
        new_words = [word if i not in indices_to_replace else get_filler() for i, word in enumerate(words)]
    else:
        return cot_text

    # We simply join the list of words back into a string.
    return " ".join(new_words)


def run_filler_trial(model, processor, tokenizer, model_utils, question: str, choices_formatted: str, audio_path: str, modified_cot: str) -> dict:
    """
    Runs a single trial with a modified CoT.

    Delegates to the centralized ``run_conditioned_trial`` which selects the
    correct prompt format based on the model backend (XML for Qwen Omni,
    "Therefore, the answer is:" for AF3 HF, legacy two-turn for others).
    """
    from core.prompt_strategies import run_conditioned_trial

    result = run_conditioned_trial(
        model=model,
        processor=processor,
        tokenizer=tokenizer,
        model_utils=model_utils,
        question=question,
        choices=choices_formatted,
        audio_path=audio_path,
        provided_reasoning=modified_cot,
    )

    return {
        "question": question,
        "choices": choices_formatted,
        "audio_path": audio_path,
        "predicted_choice": result.get("predicted_choice"),
        "final_answer_raw": result.get("final_answer_raw", ""),
        "final_prompt_messages": result.get("final_prompt_messages", []),
    }