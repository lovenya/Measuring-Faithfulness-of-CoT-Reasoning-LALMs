# data_processing/generate_spoken_filler_text.py

import os
import json
import argparse
import torch
import torchaudio
import time
import multiprocessing as mp
import numpy as np

# We import pydub, a powerful and intuitive library for audio manipulation.
# Its main job here is to create silent audio segments and stitch them
# together with the speech segments generated by our TTS model.
try:
    from pydub import AudioSegment
except ImportError:
    print("FATAL ERROR: The 'pydub' library was not found.")
    print("Please install it in your 'tts-env' Conda environment with: pip install pydub")
    exit(1)

# Standard TTS library imports, with a clear error message if the wrong environment is active.
try:
    from TTS.tts.configs.xtts_config import XttsConfig
    from TTS.tts.models.xtts import Xtts
    from TTS.utils import io
except ImportError:
    print("FATAL ERROR: The 'TTS' library was not found.")
    print("Please ensure you have activated the dedicated 'tts-env' Conda environment.")
    exit(1)

# --- Standard Setup: Monkey Patch and Model Loading ---
# This is our battle-tested setup code for the Coqui TTS model.
original_load_fsspec = io.load_fsspec
def patched_load_fsspec(path, map_location=None, **kwargs):
    kwargs['weights_only'] = False
    return original_load_fsspec(path, map_location, **kwargs)
io.load_fsspec = patched_load_fsspec

def setup_tts_model(model_dir: str) -> Xtts:
    """ Loads the local, offline Coqui XTTS-v2 model into memory. """
    print("--- Setting up Coqui TTS Model ---")
    config = XttsConfig()
    config.load_json(os.path.join(model_dir, "config.json"))
    model = Xtts.init_from_config(config)
    model.load_checkpoint(config, checkpoint_dir=model_dir, use_deepspeed=False)
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)
    print(f"--- TTS Model setup complete on device: {device} ---")
    return model

# --- Core Logic: The Speech-and-Silence Stitcher ---
def convert_filler_text_to_audio(tts_model: Xtts, text: str, speaker_wav: str) -> AudioSegment:
    """
    This is the heart of the script. It takes a string containing both normal words
    and our special '...' filler tokens and masterfully converts it into a single
    audio segment where the filler is represented by precise 40ms intervals of silence.
    """
    # We split the text by spaces. This elegantly separates words and "..." tokens
    # into a list we can loop through.
    parts = text.split(' ')
    
    # We start with an empty pydub AudioSegment. This is like an empty canvas
    # that we will progressively add our speech and silence clips to.
    final_audio = AudioSegment.empty()
    
    # To be efficient, we group consecutive words together and send them to the TTS
    # model as a single chunk, rather than one word at a time.
    current_text_chunk = ""
    for part in parts:
        if part == "...":
            # --- Handle Speech Segment ---
            # If we've accumulated a chunk of words, it's time to convert them to speech.
            if current_text_chunk.strip():
                # Generate the speech for the collected words.
                outputs = tts_model.synthesize(
                    current_text_chunk.strip(), tts_model.config, speaker_wav=speaker_wav,
                    language="en", enable_text_splitting=True
                )
                # Convert the model's output into a pydub-compatible format.
                numpy_audio = outputs['wav']
                raw_audio_data = (numpy_audio * 32767).astype(np.int16).tobytes()
                speech_segment = AudioSegment(data=raw_audio_data, sample_width=2, frame_rate=24000, channels=1)
                
                # Stitch this new speech segment onto our final audio canvas.
                final_audio += speech_segment
                current_text_chunk = "" # Reset the chunk for the next batch of words.

            # --- Handle Silence Segment ---
            # Now, create and stitch the silent segment for the "..." token.
            # This duration is not arbitrary; it's based on the Qwen2-Audio paper,
            # making our experiment methodologically sound.
            silence_segment = AudioSegment.silent(duration=40) # 40 milliseconds
            final_audio += silence_segment
        else:
            # If the part is a normal word, just add it to our current text chunk.
            if part:
                current_text_chunk += part + " "
    
    # After the loop, there might be a final chunk of text left over. We process it here.
    if current_text_chunk.strip():
        outputs = tts_model.synthesize(
            current_text_chunk.strip(), tts_model.config, speaker_wav=speaker_wav,
            language="en", enable_text_splitting=True
        )
        numpy_audio = outputs['wav']
        raw_audio_data = (numpy_audio * 32767).astype(np.int16).tobytes()
        speech_segment = AudioSegment(data=raw_audio_data, sample_width=2, frame_rate=24000, channels=1)
        final_audio += speech_segment
        
    return final_audio

def process_dataset(tts_model: Xtts, speaker_wav: str, experiment_name: str, dataset_name: str, results_dir: str, output_audio_root: str):
    """
    The main workhorse function. It orchestrates the process for a single dataset,
    looping through all trials and calling our audio generation logic.
    """
    print(f"\n--- Processing Dataset: {dataset_name.upper()} for Experiment: {experiment_name.upper()} ---")
    input_jsonl_path = os.path.join(results_dir, experiment_name, f"{experiment_name}_{dataset_name}.jsonl")
    if not os.path.exists(input_jsonl_path):
        print(f"  - WARNING: Results file not found, skipping: {input_jsonl_path}")
        return

    output_dir = os.path.join(output_audio_root, experiment_name, dataset_name)
    os.makedirs(output_dir, exist_ok=True)
    
    all_trials = [json.loads(line) for line in open(input_jsonl_path, 'r', encoding='utf-8')]
    print(f"  - Found {len(all_trials)} trials to process from: {input_jsonl_path}")
    print(f"  - Saving generated audio to: {output_dir}")

    failed_trials = []
    files_generated_this_run = 0
    for i, trial in enumerate(all_trials):
        try:
            # The reasoning text is always in the same place in our standardized results.
            text_to_speak = trial.get('final_prompt_messages', [{}, {'content': ''}])[1]['content']
            if not text_to_speak.strip():
                continue

            # We build a unique, predictable filename from the trial's metadata.
            # This is crucial for our downstream experiment scripts to find the right file.
            if 'chain_id' in trial: # Partial filler variants
                percent_key = trial['percent_replaced']
                base_name = f"{experiment_name}_{trial['id']}_{trial['chain_id']}_percent_{percent_key}"
            else: # Main filler_text experiment
                percent_key = trial['percentile']
                base_name = f"{experiment_name}_{trial['id']}_percent_{percent_key}"
            
            filename = f"{base_name}.wav"
            output_wav_path = os.path.join(output_dir, filename)

            # The HPC Restartability Feature: If the file already exists, we skip it.
            if os.path.exists(output_wav_path):
                continue
            
            if files_generated_this_run % 20 == 0:
                 print(f"  - [Trial {i+1}/{len(all_trials)}] Generating: {filename}")

            # Call our core logic to generate the speech-and-silence audio.
            generated_audio = convert_filler_text_to_audio(tts_model, text_to_speak, speaker_wav)
            
            # Export the final, stitched audio to a .wav file.
            generated_audio.export(output_wav_path, format="wav")
            files_generated_this_run += 1

        except Exception as e:
            # Our robust failure logging.
            error_message = str(e)
            failed_trials.append({"trial_index": i, "id": trial.get('id', 'N/A'), "error": error_message})
            print(f"  - âœ— ERROR on trial {i+1} (ID: {trial.get('id', 'N/A')}): {error_message}. Skipping.")
            continue
    
    # If any trials failed, we save a clean log file for later debugging.
    if failed_trials:
        log_filename = f"tts_generation_failures_{experiment_name}_{dataset_name}.log"
        log_path = os.path.join(output_dir, log_filename) # Save log in the same folder
        with open(log_path, 'w') as f:
            json.dump(failed_trials, f, indent=2)
        print(f"\n[!] WARNING: {len(failed_trials)} trials failed. A detailed log has been saved to: {log_path}")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(
        description="Generate spoken reasoning audio with silence for filler text experiments.",
        formatter_class=argparse.RawTextHelpFormatter
    )
    parser.add_argument('--experiment', type=str, required=True, choices=['filler_text', 'partial_filler_text', 'flipped_partial_filler_text', 'random_partial_filler_text'], help="The filler text experiment to process.")
    parser.add_argument('--dataset', type=str, required=True, help="The short name of the dataset to process (e.g., 'mmar' or 'all').")
    parser.add_argument('--results_dir', type=str, default='./results', help="Root directory of the 'default' condition experiment results.")
    parser.add_argument('--output_dir', type=str, default='./spoken_reasoning/audio', help="Root directory to save the generated audio files.")
    parser.add_argument('--tts_model_dir', type=str, default='./tts_models/XTTS-v2', help="Path to the local XTTS-v2 model directory.")
    parser.add_argument('--speaker_wav', type=str, default='./tts_models/reference_speaker.wav', help="Path to the reference speaker audio file.")
    args = parser.parse_args()

    tts_model = setup_tts_model(args.tts_model_dir)
    start_time = time.time()
    
    if args.dataset == 'all':
        experiment_results_dir = os.path.join(args.results_dir, args.experiment)
        if not os.path.exists(experiment_results_dir):
            print(f"FATAL: No results directory found for experiment '{args.experiment}' at '{experiment_results_dir}'")
            exit(1)
        
        result_files = [f for f in os.listdir(experiment_results_dir) if f.endswith('.jsonl') and not f.endswith(('_default.jsonl', '_transcribed_audio.jsonl'))]
        dataset_names = sorted([f.replace(f"{args.experiment}_", "").replace(".jsonl", "") for f in result_files])
        
        print(f"\nFound {len(dataset_names)} datasets to process for the '{args.experiment}' experiment: {dataset_names}")
        for dataset in dataset_names:
            process_dataset(tts_model, args.speaker_wav, args.experiment, dataset, args.results_dir, args.output_dir)
    else:
        process_dataset(tts_model, args.speaker_wav, args.experiment, args.dataset, args.results_dir, args.output_dir)
    
    end_time = time.time()
    print(f"\n--- Full generation process completed in {end_time - start_time:.2f} seconds. ---")