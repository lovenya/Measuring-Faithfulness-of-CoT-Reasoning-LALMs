# experiments/audio_interventions/audio_masking.py

"""
This script conducts the "Audio Masking" experiment, testing how models respond
when portions of the input audio are masked with silence or noise.

This is analogous to the partial filler text experiments, but operates on the
audio modality rather than the text reasoning chain.

Methodology:
1. Load pre-masked audio datasets (generated by data_processing/mask_audio_dataset.py)
2. For each percentile level (0%, 10%, 20%, ..., 100%):
   - For 0%: Copy baseline answers (hardcoded for clean consistency anchor)
   - For 10-100%: Run inference on masked audio
3. Compare answers with baseline to calculate consistency

This is an INDEPENDENT experiment - it operates on original audio, not baseline CoTs.
"""

import os
import json
import collections
import logging
from pathlib import Path
from core.prompt_strategies import get_prompt_strategy, run_reasoning_trial

# This is an 'independent' experiment - it processes original audio files
# but needs baseline results for consistency comparison
EXPERIMENT_TYPE = "independent"


def run_trial(
    model,
    processor,
    tokenizer,
    model_utils,
    question: str,
    choices: str,
    audio_path: str,
    prompt_strategy: str,
) -> dict:
    """
    Runs inference on a single masked audio sample.
    Uses the same two-turn approach as baseline but without CoT generation.
    We directly ask for the answer since we're testing audio faithfulness, not reasoning.
    """
    prompt_outputs = run_reasoning_trial(
        model=model,
        processor=processor,
        model_utils=model_utils,
        question=question,
        choices=choices,
        audio_path=audio_path,
        strategy=prompt_strategy,
    )

    parsed_choice = model_utils.parse_answer(prompt_outputs["final_answer_raw"])
    
    return {
        "question": question,
        "choices": choices,
        "audio_path": audio_path,
        "generated_cot": prompt_outputs["generated_cot"],
        "sanitized_cot": prompt_outputs["sanitized_cot"],
        "final_answer_raw": prompt_outputs["final_answer_raw"],
        "predicted_choice": parsed_choice,
        "final_prompt_messages": prompt_outputs["final_prompt_messages"],
    }


def load_baseline_results(baseline_path: str) -> dict:
    """
    Load baseline results to get the original answers for consistency comparison.
    Returns a dict keyed by (id, chain_id) -> baseline_trial_data
    """
    baseline_data = {}
    if not os.path.exists(baseline_path):
        logging.error(f"Baseline results not found at: {baseline_path}")
        return baseline_data
    
    with open(baseline_path, 'r') as f:
        for line in f:
            try:
                data = json.loads(line)
                key = (data['id'], data['chain_id'])
                baseline_data[key] = data
            except (json.JSONDecodeError, KeyError):
                continue
    
    logging.info(f"Loaded {len(baseline_data)} baseline trials from {baseline_path}")
    return baseline_data


def run(model, processor, tokenizer, model_utils, data_samples, config):
    """
    Orchestrates the Audio Masking experiment with support for:
    - Configurable mask type (silence/noise) and mode (random/start/end)
    - Iterating through percentile levels
    - 0% hardcoding for clean consistency anchor
    - Restartable design
    """
    output_path = config.OUTPUT_PATH
    mask_type = getattr(config, 'MASK_TYPE', 'silence')
    mask_mode = getattr(config, 'MASK_MODE', 'random')
    prompt_strategy = get_prompt_strategy(config)
    
    # Percentile levels to test (0% is hardcoded from baseline)
    levels = [0, 10, 20, 30, 40, 50, 60, 70, 80, 90, 100]
    
    logging.info(f"--- Running Audio Masking Experiment ---")
    logging.info(f"  Model: {config.MODEL_ALIAS.upper()}")
    logging.info(f"  Mask Type: {mask_type}")
    logging.info(f"  Mask Mode: {mask_mode}")
    logging.info(f"  Prompt Strategy: {prompt_strategy}")
    logging.info(f"  Levels: {levels}")
    logging.info(f"  Output: {output_path}")
    
    # Load baseline results for consistency comparison
    baseline_results = load_baseline_results(config.BASELINE_RESULTS_PATH)
    if not baseline_results:
        logging.error("Cannot proceed without baseline results for consistency comparison.")
        return
    
    # Get the masked dataset directory from config
    # MMAR: data/mmar_masked/{mask_type}_{mask_mode}/{level}/
    # Sakura: data/sakura/{subdataset}_masked/{mask_type}_{mask_mode}/{level}/
    dataset_name = config.DATASET_NAME
    
    # Handle sakura datasets which have a different path structure
    # Dataset name is "sakura-animal" but path is "data/sakura/animal_masked/"
    if dataset_name.startswith("sakura-"):
        subdataset = dataset_name.replace("sakura-", "")
        base_data_dir = Path(f"data/sakura/{subdataset}_masked/{mask_type}_{mask_mode}")
    else:
        base_data_dir = Path(f"data/{dataset_name}_masked/{mask_type}_{mask_mode}")
    
    if not base_data_dir.exists():
        logging.error(f"Masked dataset directory not found: {base_data_dir}")
        logging.error(f"Please generate masked datasets first using data_processing/mask_audio_dataset.py")
        return
    
    # --- Restartability Logic ---
    completed_trials = set()
    if os.path.exists(output_path):
        logging.info("Found existing results file. Checking for completed work...")
        with open(output_path, 'r') as f:
            for line in f:
                try:
                    data = json.loads(line)
                    completed_trials.add((data['id'], data['chain_id'], data['mask_percent']))
                except (json.JSONDecodeError, KeyError):
                    continue
        logging.info(f"Found {len(completed_trials)} completed trials. They will be skipped.")
    
    # --- Main Experiment Loop ---
    skipped_count = 0
    
    with open(output_path, 'a') as f:
        # Group baseline by sample ID for processing
        samples_by_id = collections.defaultdict(list)
        for (sample_id, chain_id), trial in baseline_results.items():
            samples_by_id[sample_id].append(trial)
        
        # Apply num_samples limit if specified
        sample_ids = list(samples_by_id.keys())
        if config.NUM_SAMPLES_TO_RUN > 0:
            sample_ids = sample_ids[:config.NUM_SAMPLES_TO_RUN]
        
        total_samples = len(sample_ids)
        
        for sample_idx, sample_id in enumerate(sample_ids):
            baseline_trials = samples_by_id[sample_id]
            
            # Apply chain limit
            if config.NUM_CHAINS_PER_QUESTION > 0:
                baseline_trials = [t for t in baseline_trials if t['chain_id'] < config.NUM_CHAINS_PER_QUESTION]
            
            for baseline_trial in baseline_trials:
                chain_id = baseline_trial['chain_id']
                
                for level in levels:
                    # Skip if already completed
                    if (sample_id, chain_id, level) in completed_trials:
                        continue
                    
                    try:
                        if level == 0:
                            # HARDCODE: Copy baseline answer for 0% masking
                            # This ensures a clean 100% consistency anchor
                            result = {
                                "id": sample_id,
                                "chain_id": chain_id,
                                "mask_percent": 0,
                                "mask_type": mask_type,
                                "mask_mode": mask_mode,
                                "predicted_choice": baseline_trial['predicted_choice'],
                                "correct_choice": baseline_trial['correct_choice'],
                                "is_correct": baseline_trial['is_correct'],
                                "corresponding_baseline_predicted_choice": baseline_trial['predicted_choice'],
                                "is_consistent_with_baseline": True,  # By definition
                                "final_answer_raw": baseline_trial['final_answer_raw'],
                                "audio_path": baseline_trial['audio_path'],
                                "question": baseline_trial['question'],
                                "choices": baseline_trial['choices'],
                                "note": "Hardcoded from baseline (0% masking = unchanged audio)"
                            }
                        else:
                            # Load masked audio path
                            level_dir = base_data_dir / str(level)
                            jsonl_file = list(level_dir.glob('*_standardized.jsonl'))
                            
                            if not jsonl_file:
                                logging.warning(f"No JSONL found for level {level}% in {level_dir}")
                                continue
                            
                            # Find the corresponding masked audio sample
                            masked_audio_path = None
                            with open(jsonl_file[0], 'r') as jf:
                                for line in jf:
                                    masked_sample = json.loads(line)
                                    if masked_sample['id'] == sample_id:
                                        masked_audio_path = masked_sample['audio_path']
                                        break
                            
                            if not masked_audio_path:
                                logging.warning(f"Could not find masked audio for sample {sample_id} at level {level}%")
                                continue
                            
                            if config.VERBOSE:
                                logging.info(f"Sample {sample_idx+1}/{total_samples}, ID {sample_id}, Chain {chain_id}, Level {level}%")
                            
                            # Run inference on masked audio
                            trial_result = run_trial(
                                model, processor, tokenizer, model_utils,
                                baseline_trial['question'],
                                baseline_trial['choices'],
                                masked_audio_path,
                                prompt_strategy,
                            )
                            
                            # Build final result with consistency info
                            baseline_choice = baseline_trial['predicted_choice']
                            result = {
                                "id": sample_id,
                                "chain_id": chain_id,
                                "mask_percent": level,
                                "mask_type": mask_type,
                                "mask_mode": mask_mode,
                                "predicted_choice": trial_result['predicted_choice'],
                                "correct_choice": baseline_trial['correct_choice'],
                                "is_correct": trial_result['predicted_choice'] == baseline_trial['correct_choice'],
                                "corresponding_baseline_predicted_choice": baseline_choice,
                                "is_consistent_with_baseline": trial_result['predicted_choice'] == baseline_choice,
                                "final_answer_raw": trial_result['final_answer_raw'],
                                "audio_path": masked_audio_path,
                                "question": trial_result['question'],
                                "choices": trial_result['choices'],
                                "generated_cot": trial_result['generated_cot'],
                                "sanitized_cot": trial_result['sanitized_cot'],
                            }
                        
                        f.write(json.dumps(result, ensure_ascii=False) + "\n")
                        f.flush()
                        
                    except Exception as e:
                        skipped_count += 1
                        logging.exception(f"SKIPPING trial due to error. ID: {sample_id}, Chain: {chain_id}, Level: {level}%")
                        continue
    
    # --- Final Summary ---
    logging.info(f"--- Audio Masking Experiment Complete ---")
    logging.info("\n" + "="*25 + " RUN SUMMARY " + "="*25)
    logging.info(f"Mask Type: {mask_type}")
    logging.info(f"Mask Mode: {mask_mode}")
    logging.info(f"Levels tested: {levels}")
    logging.info(f"Skipped trials due to errors: {skipped_count}")
    logging.info(f"Results saved to: {output_path}")
    logging.info("="*65)
